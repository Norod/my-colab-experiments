{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Norod/my-colab-experiments/blob/master/Buntworthy_StyleGAN2_ADA_TF1_blending_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeqkH_Ft5p4w"
      },
      "source": [
        "# Network blending in StyleGAN-ADA\n",
        "\n",
        "Swapping layers between two models in StyleGAN gives some interesting results. You need a base model and a second model which has been fine-tuned from the base.\n",
        "\n",
        "https://www.justinpinkney.com/stylegan-network-blending/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8KwDllv54v0"
      },
      "source": [
        "%cd /content/\n",
        "!git clone https://github.com/justinpinkney/stylegan2.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdbbJ2lI5-Sa"
      },
      "source": [
        "%cd stylegan2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0DKteHv6hJV"
      },
      "source": [
        "!pip uninstall tensorflow tensorflow-gpu\n",
        "!pip install tensorflow-gpu==1.14\n",
        "!pip install typer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MlAfVn8bVB"
      },
      "source": [
        "Command line help for the blending function is below. Make sure that you specify either `--output-grid` to save an example image or `--output-pkl` to save the modified pkl. (Currently only the the Gs network is modified)\n",
        "\n",
        "- low_res_pkl: Path, # Pickle file from which to take low res layers\n",
        "- high_res_pkl: Path, # Pickle file from which to take high res layers\n",
        "- resolution: int, # Resolution level at which to switch between models\n",
        "- level: int  = 0, # Switch at Conv block 0 or 1?\n",
        "- blend_width: Optional[float] = None, # None = hard switch, float = smooth switch (logistic) with given width\n",
        "- output_grid: Optional[Path] = \"blended.jpg\", # Path of image file to save example grid (None = don't save)\n",
        "- seed: int = 0, # seed for random grid\n",
        "- output_pkl: Optional[Path] = None, # Output path of pickle (None = don't save)\n",
        "- verbose: bool = False, # Print out the exact blending fraction\n",
        "\n",
        "         "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2_WTTJS6mEY"
      },
      "source": [
        "!python blend_models.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fCG-AZ69ttu"
      },
      "source": [
        "Use as an example a model I fine-tuned at 256 from FFHQ to data scraped from NASA DSCOVR:EPIC satellite (https://epic.gsfc.nasa.gov/).\n",
        "\n",
        "For the output of the fine-tuned model see the tweet below (https://twitter.com/Buntworthy/status/1295445259971899393)\n",
        "\n",
        "I'm actually going to use a model from earlier in training than the result shown in the tweet, this makes the blending a bit nicer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_rGH1tAVFvm"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzpUOVjBoiRe"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "\n",
        "%cd /content/\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada.git\n",
        "%cd stylegan2-ada\n",
        "\n",
        "!nvcc test_nvcc.cu -o test_nvcc -run\n",
        "\n",
        "print('Tensorflow version: {}'.format(tf.__version__) )\n",
        "!nvidia-smi -L\n",
        "print('GPU Identified at: {}'.format(tf.test.gpu_device_name()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3NnS2L0oqq3"
      },
      "source": [
        "!cp /content/stylegan2/blend_models.py /content/stylegan2-ada/\n",
        "!cp /content/stylegan2/training/misc.py /content/stylegan2-ada/training/\n",
        "!cp /content/stylegan2/grid_vid.py /content/stylegan2-ada/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r-q9lOkVEAn"
      },
      "source": [
        "####\n",
        "# Here you get your fine-tuned PKL and name it other.pkl\n",
        "\n",
        "!wget 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/metfaces.pkl' -O other.pkl\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGrn6dyx7MaC"
      },
      "source": [
        "####\n",
        "# Here you get your FFHQ PKL and name it ffhq.pkl\n",
        "\n",
        "!wget 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl' -O ffhq.pkl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVSbNt1F9K5O"
      },
      "source": [
        "from IPython.display import Image "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HujlFGIWVnFX"
      },
      "source": [
        "!mkdir -p \"/content/drive/My Drive/colab_data/blend\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EpOyZSB-yMF"
      },
      "source": [
        "Time to try out a bunch of different resolutions for the switch and display the results. Remember the earth model provides the low-resolution layers, i.e. the \"structure\", and the original faces model the high-resolution, i.e. the \"texture\"\n",
        "\n",
        "I'm going to run the main function in blend_models.py in a python loop, but you can also run it from the command line, something like\n",
        "\n",
        "`python blend_models.py epic-slim-256-000040.pkl stylegan2-ffhq.pkl 64 --output-grid \"blended.jpg\"`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vve_EHFl8EbQ"
      },
      "source": [
        "import blend_models\n",
        "\n",
        "resolutions = (4, 8, 16, 32, 64, 128, 256, 512)\n",
        "#resolutions = (16, 32, 64)\n",
        "for res in resolutions:\n",
        "  filename = f\"/content/drive/My Drive/colab_data/blend/other-ffhq-blended-{res}.jpg\"\n",
        "  blend_models.main(\"other.pkl\", \"ffhq.pkl\", res, output_grid=filename)\n",
        "  img = Image(filename=filename)\n",
        "  print(f\"blending at {res}x{res}\")\n",
        "  display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9MCjTAuc5IZ"
      },
      "source": [
        "import blend_models\n",
        "\n",
        "resolutions = (4, 8, 16, 32, 64, 128, 256, 512)\n",
        "#resolutions = (16, 32, 64)\n",
        "for res in resolutions:\n",
        "  filename = f\"/content/drive/My Drive/colab_data/blend/ffhq-other-blended-{res}.jpg\"\n",
        "  blend_models.main(\"ffhq.pkl\", \"other.pkl\", res, output_grid=filename)\n",
        "  img = Image(filename=filename)\n",
        "  print(f\"blending at {res}x{res}\")\n",
        "  display(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sTQ92QDd71"
      },
      "source": [
        "\n",
        "%cd /content/stylegan2-ada/\n",
        "#Here you choose the layer for the swapping in which your fine-tuned PKL is on the \"Low res\" side\n",
        "!python blend_models.py other.pkl ffhq.pkl 64 --output-pkl=\"other-ffhq-blended-64.pkl\"\n",
        "#Here you choose the layer for the swapping in which the FFHQ PKL is on the \"Low res\" side\n",
        "!python blend_models.py ffhq.pkl other.pkl 8 --output-pkl=\"ffhq-other-blended-8.pkl\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHAiguCfsLK_"
      },
      "source": [
        "#Save a copy of your blended PKLs in your drive\n",
        "!cp \"other-ffhq-blended-64.pkl\" \"/content/drive/My Drive/colab_data/blend/MetFaces-FFHQ-blended-64.pkl\"\n",
        "!cp \"ffhq-other-blended-8.pkl\"  \"/content/drive/My Drive/colab_data/blend/FFHQ-MetFaces-blended-8.pkl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ng9pNdOmyCrB"
      },
      "source": [
        "#Make interpolation animations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbrTL1XYZvh9"
      },
      "source": [
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py ffhq-other-blended-8.pkl\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/FFHQ-MetFaces-blended-8.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MworwYq9v0f4"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py \"other-ffhq-blended-64.pkl\"\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/MetFaces-FFHQ-blended-64.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mRpqQA6oUFq"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py other.pkl\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/MetFaces.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghcnUld8mmWn"
      },
      "source": [
        "#%cd /content/stylegan2/\n",
        "%cd /content/stylegan2-ada/\n",
        "import blend_models\n",
        "!python grid_vid.py ffhq.pkl\n",
        "!mv \"output.mp4\" \"/content/drive/My Drive/colab_data/blend/FFHQ.mp4\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}